#!/usr/bin/env python3
"""
å®é™…åº”ç”¨ç¤ºä¾‹é›†åˆ
å±•ç¤º Transformer ä¸­é—´çŠ¶æ€åˆ†æå™¨åœ¨çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
import time

from main import TransformerInspector
from model_loader import ModelLoader  
from token_tracker import TokenTracker
from coordinate_system import CoordinateManager

def demo_sentiment_analysis():
    """Demo 1: æƒ…æ„Ÿåˆ†æä¸­çš„ä¸­é—´è¡¨ç¤º"""
    print("ğŸ­ Demo 1: æƒ…æ„Ÿåˆ†æ - è¿½è¸ªæƒ…æ„Ÿè¯æ±‡åœ¨å„å±‚çš„è¡¨ç¤ºå˜åŒ–")
    print("="*70)
    
    inspector = TransformerInspector()
    inspector.load_model("gpt2")
    
    # å¯¹æ¯”æ­£é¢å’Œè´Ÿé¢å¥å­
    positive_text = "I absolutely love this amazing product!"
    negative_text = "I completely hate this terrible product!"
    
    # åˆ†ææ­£é¢æƒ…æ„Ÿ
    print("ğŸ˜Š åˆ†ææ­£é¢å¥å­...")
    pos_result = inspector.analyze_text(
        text=positive_text,
        target_tokens=["love", "amazing"],
        export_formats=["json"]
    )
    
    # åˆ†æè´Ÿé¢æƒ…æ„Ÿ  
    print("ğŸ˜  åˆ†æè´Ÿé¢å¥å­...")
    neg_result = inspector.analyze_text(
        text=negative_text,
        target_tokens=["hate", "terrible"],
        export_formats=["json"]
    )
    
    # æ¯”è¾ƒç»“æœ
    pos_report = pos_result['analysis_report']
    neg_report = neg_result['analysis_report']
    
    print("\nğŸ“Š å¯¹æ¯”åˆ†æ:")
    print(f"æ­£é¢å¥å­ tokens: {pos_report['summary']['tokens']}")
    print(f"è´Ÿé¢å¥å­ tokens: {neg_report['summary']['tokens']}")
    
    # åˆ†ææƒ…æ„Ÿè¯çš„éšè—çŠ¶æ€èŒƒæ•°å˜åŒ–
    print("\nğŸ’¡ æƒ…æ„Ÿè¯æ±‡çš„è¡¨ç¤ºå¼ºåº¦å˜åŒ–:")
    for pos_key, analysis in pos_report['token_analysis'].items():
        if 'hidden_state_norms' in analysis:
            norms = analysis['hidden_state_norms']
            print(f"  '{analysis['token_text']}' (æ­£é¢): å‡å€¼èŒƒæ•° = {np.mean(norms):.3f}")
    
    for pos_key, analysis in neg_report['token_analysis'].items():
        if 'hidden_state_norms' in analysis:
            norms = analysis['hidden_state_norms']
            print(f"  '{analysis['token_text']}' (è´Ÿé¢): å‡å€¼èŒƒæ•° = {np.mean(norms):.3f}")
    
    return pos_result, neg_result


def demo_word_similarity():
    """Demo 2: è¯æ±‡è¯­ä¹‰ç›¸ä¼¼æ€§åˆ†æ"""
    print("\nğŸ” Demo 2: è¯æ±‡è¯­ä¹‰ç›¸ä¼¼æ€§ - åŒä¹‰è¯åœ¨æ¨¡å‹å†…éƒ¨çš„è¡¨ç¤º")
    print("="*70)
    
    loader = ModelLoader()
    loader.load_model("gpt2")
    tracker = TokenTracker(loader.model, loader.tokenizer)
    
    # æµ‹è¯•åŒä¹‰è¯ç»„
    synonym_pairs = [
        ("The car is fast", "car", "auto"),
        ("The house is big", "big", "large"),
        ("I am happy today", "happy", "joyful")
    ]
    
    for text_template, word1, word2 in synonym_pairs:
        print(f"\nğŸ”¤ åˆ†æåŒä¹‰è¯å¯¹: '{word1}' vs '{word2}'")
        
        # åˆ›å»ºåŒ…å«ä¸¤ä¸ªè¯çš„å¥å­
        text1 = text_template.replace(word1, word1)
        text2 = text_template.replace(word1, word2)
        
        # åˆ†æç¬¬ä¸€ä¸ªè¯
        inputs1 = loader.tokenize_text(text1)
        result1 = tracker.trace_tokens(inputs1, trace_all=True)
        token_info1 = loader.get_token_info(inputs1['input_ids'])
        
        # åˆ†æç¬¬äºŒä¸ªè¯
        inputs2 = loader.tokenize_text(text2)
        result2 = tracker.trace_tokens(inputs2, trace_all=True)
        token_info2 = loader.get_token_info(inputs2['input_ids'])
        
        # æ‰¾åˆ°ç›®æ ‡è¯çš„ä½ç½®
        pos1 = None
        pos2 = None
        
        for i, token in enumerate(token_info1['tokens']):
            if word1.lower() in token.lower():
                pos1 = i
                break
                
        for i, token in enumerate(token_info2['tokens']):
            if word2.lower() in token.lower():
                pos2 = i
                break
        
        if pos1 is not None and pos2 is not None:
            # æ¯”è¾ƒç›¸åŒä½ç½®çš„è¡¨ç¤º
            print(f"  '{word1}' ä½ç½®: {pos1}, '{word2}' ä½ç½®: {pos2}")
            
            # ç®€å•çš„å±‚é—´ç›¸ä¼¼åº¦åˆ†æ
            word1_states = result1['token_states']
            word2_states = result2['token_states']
            
            similarities = []
            for layer_idx in range(12):  # GPT-2 has 12 layers
                key1 = f'position_{pos1}_layer_{layer_idx}'
                key2 = f'position_{pos2}_layer_{layer_idx}'
                
                if key1 in word1_states and key2 in word2_states:
                    h1 = word1_states[key1]['hidden_state']
                    h2 = word2_states[key2]['hidden_state']
                    
                    if h1 is not None and h2 is not None:
                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                        sim = torch.cosine_similarity(h1.flatten(), h2.flatten(), dim=0)
                        similarities.append(sim.item())
            
            if similarities:
                print(f"  å„å±‚å¹³å‡ç›¸ä¼¼åº¦: {np.mean(similarities):.4f}")
                print(f"  æœ€é«˜ç›¸ä¼¼åº¦: {max(similarities):.4f} (å±‚ {similarities.index(max(similarities))})")
                print(f"  æœ€ä½ç›¸ä¼¼åº¦: {min(similarities):.4f} (å±‚ {similarities.index(min(similarities))})")


def demo_attention_patterns():
    """Demo 3: æ³¨æ„åŠ›æ¨¡å¼åˆ†æ"""
    print("\nğŸ‘ï¸ Demo 3: æ³¨æ„åŠ›æ¨¡å¼ - åˆ†æè¯­æ³•å’Œè¯­ä¹‰å…³ç³»")
    print("="*70)
    
    loader = ModelLoader()
    loader.load_model("gpt2")
    tracker = TokenTracker(loader.model, loader.tokenizer)
    
    # åŒ…å«ä¸åŒè¯­æ³•å…³ç³»çš„å¥å­
    sentences = [
        "The red car drives quickly down the street",
        "She carefully reads the interesting book",
        "The scientist who discovered DNA won the prize"
    ]
    
    for i, text in enumerate(sentences):
        print(f"\nğŸ“– å¥å­ {i+1}: {text}")
        
        inputs = loader.tokenize_text(text)
        result = tracker.trace_tokens(inputs, trace_all=True)
        token_info = loader.get_token_info(inputs['input_ids'])
        
        print(f"  Tokens: {token_info['tokens']}")
        
        # åˆ†æåè¯å’Œå½¢å®¹è¯çš„æ³¨æ„åŠ›å…³ç³»
        nouns = []
        adjectives = []
        
        # ç®€å•çš„è¯æ€§æ£€æµ‹ï¼ˆåŸºäºå¸¸è§æ¨¡å¼ï¼‰
        noun_indicators = ["car", "book", "scientist", "DNA", "prize", "street"]
        adj_indicators = ["red", "interesting", "careful", "quick"]
        
        for j, token in enumerate(token_info['tokens']):
            if any(indicator in token.lower() for indicator in noun_indicators):
                nouns.append((j, token))
            elif any(indicator in token.lower() for indicator in adj_indicators):
                adjectives.append((j, token))
        
        print(f"  è¯†åˆ«çš„åè¯: {nouns}")
        print(f"  è¯†åˆ«çš„å½¢å®¹è¯: {adjectives}")
        
        # åˆ†æå½¢å®¹è¯å¯¹åè¯çš„æ³¨æ„åŠ›
        for adj_pos, adj_token in adjectives:
            if adj_pos < len(token_info['tokens']):
                attention_flow = tracker.get_attention_flow(adj_pos)
                
                print(f"\n  ğŸ¯ '{adj_token}' (ä½ç½® {adj_pos}) çš„æ³¨æ„åŠ›:")
                
                # æŸ¥çœ‹å‰å‡ å±‚çš„æ³¨æ„åŠ›æ¨¡å¼
                for layer in list(attention_flow['outgoing_attention'].keys())[:3]:
                    attn_info = attention_flow['outgoing_attention'][layer]
                    top_positions = attn_info['top_attended'][:3]
                    
                    attended_tokens = []
                    for pos in top_positions:
                        if pos < len(token_info['tokens']):
                            attended_tokens.append(token_info['tokens'][pos])
                    
                    print(f"    å±‚ {layer}: å…³æ³¨ â†’ {attended_tokens}")


def demo_linguistic_phenomena():
    """Demo 4: è¯­è¨€ç°è±¡åˆ†æ"""
    print("\nğŸ—£ï¸ Demo 4: è¯­è¨€ç°è±¡ - åˆ†æä¸€è¯å¤šä¹‰å’Œä¸Šä¸‹æ–‡ç†è§£")
    print("="*70)
    
    inspector = TransformerInspector()
    inspector.load_model("gpt2")
    
    # ä¸€è¯å¤šä¹‰çš„ä¾‹å­
    polysemy_examples = [
        ("The bank is closed today", "bank"),           # é“¶è¡Œ
        ("The river bank is muddy", "bank"),            # æ²³å²¸
        ("I can see the light", "light"),               # å…‰çº¿
        ("The box is very light", "light"),             # è½»çš„
        ("Time flies like an arrow", "flies"),          # é£è¡Œ
        ("Fruit flies like a banana", "flies")          # æœè‡
    ]
    
    polysemy_results = {}
    
    for text, target_word in polysemy_examples:
        print(f"\nğŸ“ åˆ†æ: {text}")
        
        result = inspector.analyze_text(
            text=text,
            target_tokens=[target_word],
            export_formats=["json"]
        )
        
        polysemy_results[text] = result
        
        # åˆ†æç›®æ ‡è¯çš„è¡¨ç¤º
        report = result['analysis_report']
        for pos_key, analysis in report['token_analysis'].items():
            if target_word in analysis['token_text']:
                norms = analysis.get('hidden_state_norms', [])
                if norms:
                    print(f"  '{analysis['token_text']}' è¡¨ç¤ºå¼ºåº¦: {np.mean(norms):.3f}")
    
    # æ¯”è¾ƒç›¸åŒè¯åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç¤º
    print(f"\nğŸ”„ ä¸€è¯å¤šä¹‰æ¯”è¾ƒåˆ†æ:")
    word_contexts = {}
    
    for text, target_word in polysemy_examples:
        if target_word not in word_contexts:
            word_contexts[target_word] = []
        word_contexts[target_word].append((text, polysemy_results[text]))
    
    for word, contexts in word_contexts.items():
        if len(contexts) > 1:
            print(f"\n  '{word}' åœ¨ä¸åŒä¸Šä¸‹æ–‡:")
            for text, result in contexts:
                report = result['analysis_report']
                for pos_key, analysis in report['token_analysis'].items():
                    if word in analysis['token_text']:
                        norms = analysis.get('hidden_state_norms', [])
                        if norms:
                            context_desc = text.replace(word, f"[{word}]")
                            print(f"    {context_desc}: å‡å€¼å¼ºåº¦ = {np.mean(norms):.3f}")


def demo_multilingual_analysis():
    """Demo 5: å¤šè¯­è¨€åˆ†æï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰"""
    print("\nğŸŒ Demo 5: å¤šè¯­è¨€æ–‡æœ¬åˆ†æ")
    print("="*70)
    
    inspector = TransformerInspector()
    inspector.load_model("gpt2")  # Note: GPT-2 ä¸»è¦æ˜¯è‹±æ–‡ï¼Œä½†æˆ‘ä»¬å¯ä»¥æµ‹è¯•å…¶ä»–å­—ç¬¦
    
    # åŒ…å«ä¸åŒå­—ç¬¦å’Œæ¦‚å¿µçš„æ–‡æœ¬
    multilingual_texts = [
        "Hello world from Earth",
        "The number 42 is special",
        "Email: user@example.com",
        "Python code: print('hello')",
        "Math: x = 2 + 2 = 4"
    ]
    
    for text in multilingual_texts:
        print(f"\nğŸ”¤ åˆ†ææ–‡æœ¬: {text}")
        
        result = inspector.analyze_text(
            text=text,
            target_tokens=None,  # åˆ†ææ‰€æœ‰ tokens
            export_formats=["json"]
        )
        
        report = result['analysis_report']
        print(f"  æ€» tokens: {report['summary']['total_tokens']}")
        print(f"  Token åˆ—è¡¨: {report['summary']['tokens']}")
        
        # åˆ†æç‰¹æ®Šå­—ç¬¦å’Œç¬¦å·çš„å¤„ç†
        for pos_key, analysis in report['token_analysis'].items():
            token_text = analysis['token_text']
            norms = analysis.get('hidden_state_norms', [])
            
            if norms:
                # æ£€æµ‹ç‰¹æ®Šå­—ç¬¦
                has_numbers = any(c.isdigit() for c in token_text)
                has_symbols = any(c in '@.=+()' for c in token_text)
                
                characteristics = []
                if has_numbers:
                    characteristics.append("æ•°å­—")
                if has_symbols:
                    characteristics.append("ç¬¦å·")
                
                char_str = f" ({','.join(characteristics)})" if characteristics else ""
                print(f"  '{token_text}'{char_str}: å‡å€¼å¼ºåº¦ = {np.mean(norms):.3f}")


def demo_performance_analysis():
    """Demo 6: æ€§èƒ½å’Œèµ„æºä½¿ç”¨åˆ†æ"""
    print("\nâš¡ Demo 6: æ€§èƒ½åˆ†æ - ç›‘æ§è®¡ç®—èµ„æºä½¿ç”¨")
    print("="*70)
    
    import psutil
    import gc
    
    inspector = TransformerInspector()
    inspector.load_model("gpt2")
    
    # æµ‹è¯•ä¸åŒé•¿åº¦æ–‡æœ¬çš„å¤„ç†æ€§èƒ½
    test_texts = [
        ("Short", "Hello world"),
        ("Medium", "The quick brown fox jumps over the lazy dog in the forest"),
        ("Long", "Natural language processing is a fascinating field that combines computer science and linguistics to help computers understand and process human language in a meaningful way, enabling applications like translation, summarization, and conversational AI")
    ]
    
    performance_results = []
    
    for name, text in test_texts:
        print(f"\nğŸ§ª æµ‹è¯• {name} æ–‡æœ¬ ({len(text.split())} è¯):")
        print(f"  æ–‡æœ¬: {text[:50]}...")
        
        # è®°å½•åˆå§‹çŠ¶æ€
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        if torch.cuda.is_available():
            start_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024
        
        # æ‰§è¡Œåˆ†æ
        result = inspector.analyze_text(
            text=text,
            target_tokens=None,  # åˆ†ææ‰€æœ‰ tokens
            export_formats=["json"]
        )
        
        # è®°å½•ç»“æŸçŠ¶æ€
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        if torch.cuda.is_available():
            end_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        duration = end_time - start_time
        memory_usage = end_memory - start_memory
        tokens_processed = result['analysis_report']['summary']['total_tokens']
        
        performance_data = {
            'name': name,
            'duration': duration,
            'memory_usage': memory_usage,
            'tokens_processed': tokens_processed,
            'tokens_per_second': tokens_processed / duration if duration > 0 else 0
        }
        
        if torch.cuda.is_available():
            gpu_memory_usage = end_gpu_memory - start_gpu_memory
            performance_data['gpu_memory_usage'] = gpu_memory_usage
            print(f"  GPU å†…å­˜ä½¿ç”¨: {gpu_memory_usage:.1f} MB")
        
        performance_results.append(performance_data)
        
        print(f"  å¤„ç†æ—¶é—´: {duration:.2f} ç§’")
        print(f"  å†…å­˜ä½¿ç”¨: {memory_usage:.1f} MB")
        print(f"  Token æ•°é‡: {tokens_processed}")
        print(f"  å¤„ç†é€Ÿåº¦: {performance_data['tokens_per_second']:.1f} tokens/ç§’")
    
    # æ€§èƒ½æ€»ç»“
    print(f"\nğŸ“ˆ æ€§èƒ½æ€»ç»“:")
    for perf in performance_results:
        print(f"  {perf['name']}: {perf['tokens_per_second']:.1f} tokens/ç§’, {perf['memory_usage']:.1f} MB å†…å­˜")
    
    return performance_results


def run_all_demos():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("ğŸš€ Transformer ä¸­é—´çŠ¶æ€åˆ†æå™¨ - å®é™…åº”ç”¨æ¼”ç¤º")
    print("="*80)
    
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path("./outputs").mkdir(exist_ok=True)
        
        # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
        demo_sentiment_analysis()
        demo_word_similarity()
        demo_attention_patterns()
        demo_linguistic_phenomena()
        demo_multilingual_analysis()
        demo_performance_analysis()
        
        print("\n" + "="*80)
        print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("ğŸ“ æŸ¥çœ‹ ./outputs ç›®å½•è·å–è¯¦ç»†åˆ†æç»“æœ")
        print("ğŸ’¡ è¿™äº›æ¼”ç¤ºå±•ç¤ºäº†å·¥å…·åœ¨å®é™… NLP ç ”ç©¶ä¸­çš„åº”ç”¨æ½œåŠ›")
        print("="*80)
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import sys
    
    demos = {
        1: demo_sentiment_analysis,
        2: demo_word_similarity, 
        3: demo_attention_patterns,
        4: demo_linguistic_phenomena,
        5: demo_multilingual_analysis,
        6: demo_performance_analysis
    }
    
    if len(sys.argv) > 1:
        demo_num = int(sys.argv[1])
        if demo_num in demos:
            demos[demo_num]()
        else:
            print(f"æ¼”ç¤º {demo_num} ä¸å­˜åœ¨")
            print(f"å¯ç”¨æ¼”ç¤º: {list(demos.keys())}")
    else:
        run_all_demos()