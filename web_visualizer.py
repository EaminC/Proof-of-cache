#!/usr/bin/env python3
"""
Webå¯è§†åŒ–æœåŠ¡å™¨
æä¾›äº¤äº’å¼çš„Transformerä¸­é—´çŠ¶æ€å¯è§†åŒ–ç•Œé¢
"""

from flask import Flask, render_template, request, jsonify, send_from_directory
from flask_cors import CORS
import json
import numpy as np
import torch
import traceback
from pathlib import Path
import gzip
import time

# å¯¼å…¥æˆ‘ä»¬çš„åˆ†æå™¨
from main import TransformerInspector
from model_loader import ModelLoader
from token_tracker import TokenTracker
from coordinate_system import CoordinateManager

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€å˜é‡å­˜å‚¨å½“å‰åˆ†æçŠ¶æ€
current_inspector = None
current_analysis_result = None
current_token_info = None

@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    return render_template('index.html')

@app.route('/api/load_model', methods=['POST'])
def load_model():
    """åŠ è½½æ¨¡å‹API"""
    global current_inspector
    
    try:
        data = request.get_json()
        model_name = data.get('model_name', 'gpt2')
        
        # åˆ›å»ºåˆ†æå™¨å¹¶åŠ è½½æ¨¡å‹
        current_inspector = TransformerInspector()
        current_inspector.load_model(model_name)
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = {
            'model_type': model_name,
            'num_parameters': current_inspector.loader.model.num_parameters(),
            'num_layers': current_inspector.loader.model.config.n_layer,
            'num_heads': current_inspector.loader.model.config.n_head,
            'hidden_size': current_inspector.loader.model.config.n_embd,
            'vocab_size': current_inspector.loader.model.config.vocab_size
        }
        
        return jsonify({
            'success': True,
            'model_info': model_info,
            'message': f'æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ'
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        })

@app.route('/api/analyze_text', methods=['POST'])
def analyze_text():
    """åˆ†ææ–‡æœ¬API"""
    global current_inspector, current_analysis_result, current_token_info
    
    try:
        if current_inspector is None:
            return jsonify({
                'success': False,
                'error': 'è¯·å…ˆåŠ è½½æ¨¡å‹'
            })
        
        data = request.get_json()
        text = data.get('text', '')
        target_tokens = data.get('target_tokens', None)
        
        if not text.strip():
            return jsonify({
                'success': False,
                'error': 'è¯·è¾“å…¥æ–‡æœ¬'
            })
        
        # æ‰§è¡Œåˆ†æ
        result = current_inspector.analyze_text(
            text=text,
            target_tokens=target_tokens,
            export_formats=["json"]
        )
        
        current_analysis_result = result
        current_token_info = result['token_info']
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            'success': True,
            'tokens': current_token_info['tokens'],
            'token_ids': current_token_info['token_ids'].tolist(),
            'analysis_summary': result['analysis_report']['summary'],
            'token_analysis': result['analysis_report']['token_analysis'],
            'coordinate_summary': result['coordinate_summary']
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        })

@app.route('/api/get_token_details/<int:token_position>')
def get_token_details(token_position):
    """è·å–ç‰¹å®štokençš„è¯¦ç»†ä¿¡æ¯"""
    global current_analysis_result, current_token_info
    
    try:
        if current_analysis_result is None:
            return jsonify({
                'success': False,
                'error': 'è¯·å…ˆè¿›è¡Œæ–‡æœ¬åˆ†æ'
            })
        
        if token_position >= len(current_token_info['tokens']):
            return jsonify({
                'success': False,
                'error': 'Tokenä½ç½®è¶…å‡ºèŒƒå›´'
            })
        
        # è·å–è¯¥tokençš„è¯¦ç»†åˆ†æ
        position_key = f'position_{token_position}'
        token_analysis = current_analysis_result['analysis_report']['token_analysis'].get(position_key, {})
        
        # è·å–coordinate managerä¸­çš„ç›¸å…³å˜é‡
        coord_manager = current_inspector.coordinate_manager
        position_vars = coord_manager.get_position_variables(token_position)
        
        # æ„å»ºå±‚çº§æ•°æ®
        layer_data = {}
        for layer_idx in range(current_inspector.loader.model.config.n_layer):
            layer_vars = coord_manager.get_layer_variables(layer_idx)
            position_layer_vars = [v for v in layer_vars if v['coordinate'].sequence_idx == token_position]
            
            layer_info = {
                'layer_index': layer_idx,
                'variables': [],
                'hidden_state_norm': None,
                'attention_weights': None
            }
            
            for var_info in position_layer_vars:
                coord = var_info['coordinate']
                value = var_info['value']
                
                var_data = {
                    'module_type': coord.module_type,
                    'module_name': coord.module_name,
                    'shape': list(value.shape) if hasattr(value, 'shape') else [],
                    'norm': float(torch.norm(value)) if hasattr(value, 'norm') else None,
                    'mean': float(torch.mean(value)) if hasattr(value, 'mean') else None,
                    'std': float(torch.std(value)) if hasattr(value, 'std') else None
                }
                
                if coord.module_type == 'hidden_state':
                    layer_info['hidden_state_norm'] = var_data['norm']
                elif coord.module_type == 'attention' and coord.head_idx is not None:
                    if layer_info['attention_weights'] is None:
                        layer_info['attention_weights'] = {}
                    layer_info['attention_weights'][coord.head_idx] = var_data
                
                layer_info['variables'].append(var_data)
            
            layer_data[layer_idx] = layer_info
        
        # è·å–æ³¨æ„åŠ›æµä¿¡æ¯
        attention_flow = {}
        if hasattr(current_inspector.tracker, 'get_attention_flow'):
            try:
                attention_flow = current_inspector.tracker.get_attention_flow(token_position)
            except:
                pass
        
        response_data = {
            'success': True,
            'token_position': token_position,
            'token_text': current_token_info['tokens'][token_position],
            'token_id': int(current_token_info['token_ids'][token_position]),
            'layer_data': layer_data,
            'attention_flow': attention_flow,
            'analysis': token_analysis
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        })

@app.route('/api/get_attention_matrix')
def get_attention_matrix():
    """è·å–å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ"""
    global current_analysis_result
    
    try:
        if current_analysis_result is None:
            return jsonify({
                'success': False,
                'error': 'è¯·å…ˆè¿›è¡Œæ–‡æœ¬åˆ†æ'
            })
        
        # ä»trackingç»“æœä¸­è·å–æ³¨æ„åŠ›æ¨¡å¼
        tracking_result = current_analysis_result['tracking_result']
        attention_patterns = tracking_result.get('attention_patterns', {})
        
        # æ„å»ºæ³¨æ„åŠ›çŸ©é˜µæ•°æ®
        attention_matrices = {}
        
        for layer_key, layer_attention in attention_patterns.items():
            if 'attention_weights' in layer_attention:
                # è½¬æ¢æ³¨æ„åŠ›æƒé‡ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                attention_weights = layer_attention['attention_weights']
                if torch.is_tensor(attention_weights):
                    # å‡è®¾å½¢çŠ¶ä¸º [num_heads, seq_len, seq_len]
                    attention_matrices[layer_key] = {
                        'weights': attention_weights.detach().cpu().numpy().tolist(),
                        'shape': list(attention_weights.shape)
                    }
        
        return jsonify({
            'success': True,
            'attention_matrices': attention_matrices,
            'sequence_length': len(current_token_info['tokens'])
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        })

@app.route('/api/get_model_structure')
def get_model_structure():
    """è·å–æ¨¡å‹ç»“æ„ä¿¡æ¯"""
    global current_inspector
    
    try:
        if current_inspector is None:
            return jsonify({
                'success': False,
                'error': 'è¯·å…ˆåŠ è½½æ¨¡å‹'
            })
        
        model = current_inspector.loader.model
        
        # æ„å»ºæ¨¡å‹ç»“æ„æ ‘
        structure = {
            'model_type': model.config.model_type,
            'num_layers': model.config.n_layer,
            'num_heads': model.config.n_head,
            'hidden_size': model.config.n_embd,
            'layers': []
        }
        
        for i in range(model.config.n_layer):
            layer_info = {
                'index': i,
                'name': f'transformer.h.{i}',
                'components': [
                    {
                        'name': 'attention',
                        'type': 'multi_head_attention',
                        'num_heads': model.config.n_head,
                        'head_dim': model.config.n_embd // model.config.n_head
                    },
                    {
                        'name': 'mlp',
                        'type': 'feedforward',
                        'hidden_size': model.config.n_embd * 4
                    },
                    {
                        'name': 'ln_1',
                        'type': 'layer_norm'
                    },
                    {
                        'name': 'ln_2', 
                        'type': 'layer_norm'
                    }
                ]
            }
            structure['layers'].append(layer_info)
        
        return jsonify({
            'success': True,
            'structure': structure
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        })

@app.route('/static/<path:filename>')
def serve_static(filename):
    """æä¾›é™æ€æ–‡ä»¶"""
    return send_from_directory('static', filename)

if __name__ == '__main__':
    import os
    import socket
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    Path('templates').mkdir(exist_ok=True)
    Path('static').mkdir(exist_ok=True)
    Path('static/css').mkdir(exist_ok=True)
    Path('static/js').mkdir(exist_ok=True)
    
    # è·å–æœ¬æœºIPåœ°å€
    hostname = socket.gethostname()
    local_ip = socket.gethostbyname(hostname)
    
    # ç«¯å£é…ç½®
    port = int(os.environ.get('PORT', 5000))
    
    print("ğŸš€ å¯åŠ¨Transformerå¯è§†åŒ–æœåŠ¡å™¨...")
    print("ğŸ“ æœ¬åœ°è®¿é—®: http://localhost:{}".format(port))
    print("ğŸŒ å†…ç½‘è®¿é—®: http://{}:{}".format(local_ip, port))
    print("ğŸŒ å¤–éƒ¨è®¿é—®: http://192.5.86.157:{}".format(port))
    print("ğŸ’¡ å¦‚æœå¤–éƒ¨æ— æ³•è®¿é—®ï¼Œè¯·æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")
    print("")
    
    try:
        app.run(host='0.0.0.0', port=port, debug=True, threaded=True)
    except OSError as e:
        if "Address already in use" in str(e):
            print(f"âŒ ç«¯å£ {port} å·²è¢«å ç”¨ï¼Œå°è¯•ä½¿ç”¨ç«¯å£ {port+1}")
            app.run(host='0.0.0.0', port=port+1, debug=True, threaded=True)
        else:
            raise e